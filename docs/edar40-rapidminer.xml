<?xml version="1.0" encoding="UTF-8"?><radoop-connection-entry>
  <name>EDAR (smvhortonworks)</name>
  <compatibilityLevel>9.1.0</compatibilityLevel>
  <masterAddress/>
  <jobtrackerAddress>smvhortonworks</jobtrackerAddress>
  <namenodeAddress>smvhortonworks</namenodeAddress>
  <jobHistoryServerAddress>smvhortonworks</jobHistoryServerAddress>
  <hiveserverAddress>smvhortonworks</hiveserverAddress>
  <multipleMasterAddress>T</multipleMasterAddress>
  <hiveDB>epele</hiveDB>
  <hivePort>10000</hivePort>
  <mapredPort>8050</mapredPort>
  <hdfsPort>8020</hdfsPort>
  <jobHistoryServerPort>10020</jobHistoryServerPort>
  <hiveHighAvailability>F</hiveHighAvailability>
  <zookeeperQuorum>smvhortonworks:2181</zookeeperQuorum>
  <zookeeperNamespace>hiveserver2</zookeeperNamespace>
  <hadoopVersion>hadoop-hortonworks-hdp-2.x</hadoopVersion>
  <useDefaultPorts>F</useDefaultPorts>
  <useRadoopProxy>F</useRadoopProxy>
  <securityEnabled>T</securityEnabled>
  <retrievePrincipalsFromHive>F</retrievePrincipalsFromHive>
  <radoopProxyName/>
  <radoopProxySource/>
  <realm>EDAR40.EUS</realm>
  <kdc>smvhortonworks</kdc>
  <krbConfFile/>
  <saslQopLevel>auth</saslQopLevel>
  <clientPrincipal></clientPrincipal>
  <hdfsCertificate>nn/_HOST@EDAR40.EUS</hdfsCertificate>
  <mapredCertificate>rm/_HOST@EDAR40.EUS</mapredCertificate>
  <hivePrincipal>hive/__hiveserver_hostname__@EDAR40.EUS</hivePrincipal>
  <jobHistoryServerPrincipal>jhs/_HOST@EDAR40.EUS</jobHistoryServerPrincipal>
  <keytabFile></keytabFile>
  <usekerberospassword>F</usekerberospassword>
  <kerberospassword>zz8MX9PUxAI=_</kerberospassword>
  <impersonation>F</impersonation>
  <localTestImpersonatedUser/>
  <sparkVersion>SPARK_HDP_23_1</sparkVersion>
  <useCustomPySparkLocation>F</useCustomPySparkLocation>
  <useCustomSparkRLocation>F</useCustomSparkRLocation>
  <customPySparkLocation/>
  <customSparkRLocation/>
  <sparkAssemblyJar>local:///usr/hdp/current/spark2-client/jars/*</sparkAssemblyJar>
  <sparkResourceAllocationPolicy>static_default</sparkResourceAllocationPolicy>
  <sparkHeuristicAllocationPercentage>30</sparkHeuristicAllocationPercentage>
  <advancedHadoopSettings>
    <keyvalueenabledelement>
      <key>dfs.block.access.token.enable</key>
      <valuee>true</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>dfs.blockreport.initialDelay</key>
      <valuee>120</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>dfs.blocksize</key>
      <valuee>134217728</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>dfs.client.block.write.replace-datanode-on-failure.policy</key>
      <valuee>NEVER</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>dfs.client.read.shortcircuit</key>
      <valuee>true</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>dfs.client.read.shortcircuit.streams.cache.size</key>
      <valuee>4096</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>dfs.client.retry.policy.enabled</key>
      <valuee>false</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>dfs.cluster.administrators</key>
      <valuee> hdfs</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>dfs.content-summary.limit</key>
      <valuee>5000</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>dfs.datanode.address</key>
      <valuee>0.0.0.0:1019</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>dfs.datanode.balance.bandwidthPerSec</key>
      <valuee>6250000</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>dfs.datanode.data.dir</key>
      <valuee>/hadoop/hdfs/data</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>dfs.datanode.data.dir.perm</key>
      <valuee>750</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>dfs.datanode.du.reserved</key>
      <valuee>5135450112</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>dfs.datanode.failed.volumes.tolerated</key>
      <valuee>0</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>dfs.datanode.http.address</key>
      <valuee>0.0.0.0:1022</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>dfs.datanode.https.address</key>
      <valuee>0.0.0.0:50475</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>dfs.datanode.ipc.address</key>
      <valuee>0.0.0.0:8010</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>dfs.datanode.kerberos.principal</key>
      <valuee>dn/_HOST@EDAR40.EUS</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>dfs.datanode.keytab.file</key>
      <valuee>/etc/security/keytabs/dn.service.keytab</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>dfs.datanode.max.transfer.threads</key>
      <valuee>4096</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>dfs.domain.socket.path</key>
      <valuee>/var/lib/hadoop-hdfs/dn_socket</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>dfs.encrypt.data.transfer.cipher.suites</key>
      <valuee>AES/CTR/NoPadding</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>dfs.heartbeat.interval</key>
      <valuee>3</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>dfs.hosts.exclude</key>
      <valuee>/etc/hadoop/conf/dfs.exclude</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>dfs.http.policy</key>
      <valuee>HTTP_ONLY</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>dfs.https.enable</key>
      <valuee>false</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>dfs.https.port</key>
      <valuee>50470</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>dfs.journalnode.edits.dir</key>
      <valuee>/hadoop/hdfs/journalnode</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>dfs.journalnode.http-address</key>
      <valuee>0.0.0.0:8480</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>dfs.journalnode.https-address</key>
      <valuee>0.0.0.0:8481</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>dfs.namenode.accesstime.precision</key>
      <valuee>0</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>dfs.namenode.audit.log.async</key>
      <valuee>true</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>dfs.namenode.avoid.read.stale.datanode</key>
      <valuee>true</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>dfs.namenode.avoid.write.stale.datanode</key>
      <valuee>true</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>dfs.namenode.checkpoint.dir</key>
      <valuee>/hadoop/hdfs/namesecondary</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>dfs.namenode.checkpoint.edits.dir</key>
      <valuee>${dfs.namenode.checkpoint.dir}</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>dfs.namenode.checkpoint.period</key>
      <valuee>21600</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>dfs.namenode.checkpoint.txns</key>
      <valuee>1000000</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>dfs.namenode.fslock.fair</key>
      <valuee>false</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>dfs.namenode.handler.count</key>
      <valuee>50</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>dfs.namenode.http-address</key>
      <valuee>smvhortonworks:50070</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>dfs.namenode.https-address</key>
      <valuee>smvhortonworks:50470</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>dfs.namenode.inode.attributes.provider.class</key>
      <valuee>org.apache.ranger.authorization.hadoop.RangerHdfsAuthorizer</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>dfs.namenode.kerberos.internal.spnego.principal</key>
      <valuee>HTTP/_HOST@EDAR40.EUS</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>dfs.namenode.kerberos.principal</key>
      <valuee>nn/_HOST@EDAR40.EUS</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>dfs.namenode.keytab.file</key>
      <valuee>/etc/security/keytabs/nn.service.keytab</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>dfs.namenode.name.dir</key>
      <valuee>/hadoop/hdfs/namenode</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>dfs.namenode.name.dir.restore</key>
      <valuee>true</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>dfs.namenode.rpc-address</key>
      <valuee>smvhortonworks:8020</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>dfs.namenode.safemode.threshold-pct</key>
      <valuee>1</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>dfs.namenode.secondary.http-address</key>
      <valuee>smvhortonworks:50090</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>dfs.namenode.secondary.https-address</key>
      <valuee>smvhortonworks:50092</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>dfs.namenode.stale.datanode.interval</key>
      <valuee>30000</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>dfs.namenode.startup.delay.block.deletion.sec</key>
      <valuee>3600</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>dfs.namenode.write.stale.datanode.ratio</key>
      <valuee>1.0f</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>dfs.permissions.enabled</key>
      <valuee>true</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>dfs.permissions.superusergroup</key>
      <valuee>hdfs</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>dfs.replication</key>
      <valuee>3</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>dfs.replication.max</key>
      <valuee>50</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>dfs.secondary.namenode.kerberos.internal.spnego.principal</key>
      <valuee>HTTP/_HOST@EDAR40.EUS</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>dfs.secondary.namenode.kerberos.principal</key>
      <valuee>nn/_HOST@EDAR40.EUS</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>dfs.secondary.namenode.keytab.file</key>
      <valuee>/etc/security/keytabs/nn.service.keytab</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>dfs.support.append</key>
      <valuee>true</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>dfs.web.authentication.kerberos.keytab</key>
      <valuee>/etc/security/keytabs/spnego.service.keytab</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>dfs.web.authentication.kerberos.principal</key>
      <valuee>HTTP/_HOST@EDAR40.EUS</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>dfs.webhdfs.enabled</key>
      <valuee>true</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>fs.azure.user.agent.prefix</key>
      <valuee>User-Agent: APN/1.0 Hortonworks/1.0 HDP/{{version}}</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>fs.defaultFS</key>
      <valuee>hdfs://smvhortonworks:8020</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>fs.permissions.umask-mode</key>
      <valuee>022</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>fs.s3a.user.agent.prefix</key>
      <valuee>User-Agent: APN/1.0 Hortonworks/1.0 HDP/{{version}}</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>fs.trash.interval</key>
      <valuee>360</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>ha.failover-controller.active-standby-elector.zk.op.retries</key>
      <valuee>120</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>ha.zookeeper.acl</key>
      <valuee>sasl:nn:rwcda</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>hadoop.caller.context.enabled</key>
      <valuee>true</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>hadoop.custom-extensions.root</key>
      <valuee>/hdp/ext/{{major_stack_version}}/hadoop</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>hadoop.http.authentication.simple.anonymous.allowed</key>
      <valuee>true</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>hadoop.proxyuser.ambari-server-edar.groups</key>
      <valuee>*</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>hadoop.proxyuser.ambari-server-edar.hosts</key>
      <valuee>smvhortonworks</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>hadoop.proxyuser.hcat.groups</key>
      <valuee>*</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>hadoop.proxyuser.hcat.hosts</key>
      <valuee>smvhortonworks</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>hadoop.proxyuser.hdfs.groups</key>
      <valuee>*</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>hadoop.proxyuser.hdfs.hosts</key>
      <valuee>*</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>hadoop.proxyuser.hive.groups</key>
      <valuee>*</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>hadoop.proxyuser.hive.hosts</key>
      <valuee>smvhortonworks</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>hadoop.proxyuser.HTTP.groups</key>
      <valuee>users</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>hadoop.proxyuser.HTTP.hosts</key>
      <valuee>smvhortonworks</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>hadoop.proxyuser.knox.groups</key>
      <valuee>*</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>hadoop.proxyuser.knox.hosts</key>
      <valuee>smvhortonworks</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>hadoop.proxyuser.livy.groups</key>
      <valuee>*</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>hadoop.proxyuser.livy.hosts</key>
      <valuee>*</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>hadoop.proxyuser.oozie.groups</key>
      <valuee>*</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>hadoop.proxyuser.oozie.hosts</key>
      <valuee>smvhortonworks</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>hadoop.proxyuser.yarn.groups</key>
      <valuee>*</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>hadoop.proxyuser.yarn.hosts</key>
      <valuee>smvhortonworks</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>hadoop.proxyuser.zeppelin-edar.groups</key>
      <valuee>*</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>hadoop.proxyuser.zeppelin-edar.hosts</key>
      <valuee>*</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>hadoop.proxyuser.zeppelin.groups</key>
      <valuee>*</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>hadoop.proxyuser.zeppelin.hosts</key>
      <valuee>*</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>hadoop.registry.client.auth</key>
      <valuee>kerberos</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>hadoop.registry.jaas.context</key>
      <valuee>Client</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>hadoop.registry.rm.enabled</key>
      <valuee>true</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>hadoop.registry.secure</key>
      <valuee>true</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>hadoop.registry.system.accounts</key>
      <valuee>sasl:yarn,sasl:jhs,sasl:hdfs-edar,sasl:rm,sasl:hive</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>hadoop.registry.zk.quorum</key>
      <valuee>smvhortonworks:2181</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>hadoop.security.auth_to_local</key>
      <valuee>RULE:[1:$1@$0](ambari-qa-edar@EDAR40.EUS)s/.*/ambari-qa/
RULE:[1:$1@$0](druid@EDAR40.EUS)s/.*/druid/
RULE:[1:$1@$0](hbase-edar@EDAR40.EUS)s/.*/hbase/
RULE:[1:$1@$0](hdfs-edar@EDAR40.EUS)s/.*/hdfs/
RULE:[1:$1@$0](spark-edar@EDAR40.EUS)s/.*/spark/
RULE:[1:$1@$0](zeppelin-edar@EDAR40.EUS)s/.*/zeppelin/
RULE:[1:$1@$0](.*@EDAR40.EUS)s/@.*//
RULE:[2:$1@$0](amshbase@EDAR40.EUS)s/.*/ams/
RULE:[2:$1@$0](amszk@EDAR40.EUS)s/.*/ams/
RULE:[2:$1@$0](dn@EDAR40.EUS)s/.*/hdfs/
RULE:[2:$1@$0](hbase@EDAR40.EUS)s/.*/hbase/
RULE:[2:$1@$0](hive@EDAR40.EUS)s/.*/hive/
RULE:[2:$1@$0](jhs@EDAR40.EUS)s/.*/mapred/
RULE:[2:$1@$0](jn@EDAR40.EUS)s/.*/hdfs/
RULE:[2:$1@$0](knox@EDAR40.EUS)s/.*/knox/
RULE:[2:$1@$0](livy@EDAR40.EUS)s/.*/livy/
RULE:[2:$1@$0](nfs@EDAR40.EUS)s/.*/hdfs/
RULE:[2:$1@$0](nifi@EDAR40.EUS)s/.*/nifi/
RULE:[2:$1@$0](nm@EDAR40.EUS)s/.*/yarn/
RULE:[2:$1@$0](nn@EDAR40.EUS)s/.*/hdfs/
RULE:[2:$1@$0](rangeradmin@EDAR40.EUS)s/.*/ranger/
RULE:[2:$1@$0](rangertagsync@EDAR40.EUS)s/.*/rangertagsync/
RULE:[2:$1@$0](rangerusersync@EDAR40.EUS)s/.*/rangerusersync/
RULE:[2:$1@$0](rm@EDAR40.EUS)s/.*/yarn/
RULE:[2:$1@$0](yarn@EDAR40.EUS)s/.*/yarn/
DEFAULT</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>hadoop.security.authentication</key>
      <valuee>kerberos</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>hadoop.security.authorization</key>
      <valuee>true</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>hadoop.ssl.client-conf</key>
      <valuee>ssl-client.xml</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>hadoop.ssl.hostname.verifier</key>
      <valuee>DEFAULT</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>hadoop.ssl.keystores.factory.class</key>
      <valuee>org.apache.hadoop.security.ssl.FileBasedKeyStoresFactory</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>hadoop.ssl.require.client.cert</key>
      <valuee>false</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>hadoop.ssl.server.conf</key>
      <valuee>ssl-server.xml</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>io.compression.codecs</key>
      <valuee>org.apache.hadoop.io.compress.GzipCodec,org.apache.hadoop.io.compress.DefaultCodec,org.apache.hadoop.io.compress.SnappyCodec</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>io.file.buffer.size</key>
      <valuee>131072</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>io.serializations</key>
      <valuee>org.apache.hadoop.io.serializer.WritableSerialization</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>ipc.client.connect.max.retries</key>
      <valuee>50</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>ipc.client.connection.maxidletime</key>
      <valuee>30000</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>ipc.client.idlethreshold</key>
      <valuee>8000</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>ipc.server.tcpnodelay</key>
      <valuee>true</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>manage.include.files</key>
      <valuee>false</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>mapreduce.admin.map.child.java.opts</key>
      <valuee>-server -XX:NewRatio=8 -Djava.net.preferIPv4Stack=true -Dhdp.version=${hdp.version}</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>mapreduce.admin.reduce.child.java.opts</key>
      <valuee>-server -XX:NewRatio=8 -Djava.net.preferIPv4Stack=true -Dhdp.version=${hdp.version}</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>mapreduce.admin.user.env</key>
      <valuee>LD_LIBRARY_PATH={{hadoop_lib_home}}/native:{{hadoop_lib_home}}/native/Linux-{{architecture}}-64:./mr-framework/hadoop/lib/native:./mr-framework/hadoop/lib/native/Linux-{{architecture}}-64</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>mapreduce.am.max-attempts</key>
      <valuee>2</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>mapreduce.application.classpath</key>
      <valuee>$PWD/mr-framework/hadoop/share/hadoop/mapreduce/*:$PWD/mr-framework/hadoop/share/hadoop/mapreduce/lib/*:$PWD/mr-framework/hadoop/share/hadoop/common/*:$PWD/mr-framework/hadoop/share/hadoop/common/lib/*:$PWD/mr-framework/hadoop/share/hadoop/yarn/*:$PWD/mr-framework/hadoop/share/hadoop/yarn/lib/*:$PWD/mr-framework/hadoop/share/hadoop/hdfs/*:$PWD/mr-framework/hadoop/share/hadoop/hdfs/lib/*:$PWD/mr-framework/hadoop/share/hadoop/tools/lib/*:/usr/hdp/${hdp.version}/hadoop/lib/hadoop-lzo-0.6.0.${hdp.version}.jar:/etc/hadoop/conf/secure:/usr/hdp/current/ext/hadoop/*</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>mapreduce.application.framework.path</key>
      <valuee>/hdp/apps/${hdp.version}/mapreduce/mapreduce.tar.gz#mr-framework</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>mapreduce.cluster.administrators</key>
      <valuee> hadoop</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>mapreduce.framework.name</key>
      <valuee>yarn</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>mapreduce.job.counters.max</key>
      <valuee>130</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>mapreduce.job.emit-timeline-data</key>
      <valuee>false</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>mapreduce.job.queuename</key>
      <valuee>default</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>mapreduce.job.reduce.slowstart.completedmaps</key>
      <valuee>0.05</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>mapreduce.jobhistory.address</key>
      <valuee>smvhortonworks:10020</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>mapreduce.jobhistory.bind-host</key>
      <valuee>0.0.0.0</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>mapreduce.jobhistory.done-dir</key>
      <valuee>/mr-history/done</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>mapreduce.jobhistory.http.policy</key>
      <valuee>HTTP_ONLY</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>mapreduce.jobhistory.intermediate-done-dir</key>
      <valuee>/mr-history/tmp</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>mapreduce.jobhistory.keytab</key>
      <valuee>/etc/security/keytabs/jhs.service.keytab</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>mapreduce.jobhistory.principal</key>
      <valuee>jhs/_HOST@EDAR40.EUS</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>mapreduce.jobhistory.recovery.enable</key>
      <valuee>true</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>mapreduce.jobhistory.recovery.store.class</key>
      <valuee>org.apache.hadoop.mapreduce.v2.hs.HistoryServerLeveldbStateStoreService</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>mapreduce.jobhistory.recovery.store.leveldb.path</key>
      <valuee>/hadoop/mapreduce/jhs</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>mapreduce.jobhistory.webapp.address</key>
      <valuee>smvhortonworks:19888</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>mapreduce.jobhistory.webapp.https.address</key>
      <valuee>smvhortonworks:19890</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>mapreduce.jobhistory.webapp.spnego-keytab-file</key>
      <valuee>/etc/security/keytabs/spnego.service.keytab</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>mapreduce.jobhistory.webapp.spnego-principal</key>
      <valuee>HTTP/_HOST@EDAR40.EUS</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>mapreduce.jobtracker.webinterface.trusted</key>
      <valuee>false</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>mapreduce.map.java.opts</key>
      <valuee>-Xmx1843m</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>mapreduce.map.log.level</key>
      <valuee>INFO</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>mapreduce.map.memory.mb</key>
      <valuee>2304</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>mapreduce.map.output.compress</key>
      <valuee>false</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>mapreduce.map.sort.spill.percent</key>
      <valuee>0.7</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>mapreduce.map.speculative</key>
      <valuee>false</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>mapreduce.output.fileoutputformat.compress</key>
      <valuee>false</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>mapreduce.output.fileoutputformat.compress.type</key>
      <valuee>BLOCK</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>mapreduce.reduce.input.buffer.percent</key>
      <valuee>0.0</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>mapreduce.reduce.java.opts</key>
      <valuee>-Xmx3686m</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>mapreduce.reduce.log.level</key>
      <valuee>INFO</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>mapreduce.reduce.memory.mb</key>
      <valuee>4608</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>mapreduce.reduce.shuffle.fetch.retry.enabled</key>
      <valuee>1</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>mapreduce.reduce.shuffle.fetch.retry.interval-ms</key>
      <valuee>1000</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>mapreduce.reduce.shuffle.fetch.retry.timeout-ms</key>
      <valuee>30000</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>mapreduce.reduce.shuffle.input.buffer.percent</key>
      <valuee>0.7</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>mapreduce.reduce.shuffle.merge.percent</key>
      <valuee>0.66</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>mapreduce.reduce.shuffle.parallelcopies</key>
      <valuee>30</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>mapreduce.reduce.speculative</key>
      <valuee>false</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>mapreduce.shuffle.port</key>
      <valuee>13562</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>mapreduce.shuffle.ssl.enabled</key>
      <valuee>true</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>mapreduce.ssl.enabled</key>
      <valuee>true</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>mapreduce.task.io.sort.factor</key>
      <valuee>100</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>mapreduce.task.io.sort.mb</key>
      <valuee>1290</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>mapreduce.task.timeout</key>
      <valuee>300000</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>net.topology.script.file.name</key>
      <valuee>/etc/hadoop/conf/topology_script.py</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>nfs.exports.allowed.hosts</key>
      <valuee>* rw</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>nfs.file.dump.dir</key>
      <valuee>/tmp/.hdfs-nfs</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>yarn.acl.enable</key>
      <valuee>true</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>yarn.admin.acl</key>
      <valuee>yarn,dr.who</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>yarn.app.mapreduce.am.admin-command-opts</key>
      <valuee>-Dhdp.version=${hdp.version}</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>yarn.app.mapreduce.am.command-opts</key>
      <valuee>-Xmx1843m -Dhdp.version=${hdp.version}</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>yarn.app.mapreduce.am.log.level</key>
      <valuee>INFO</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>yarn.app.mapreduce.am.resource.mb</key>
      <valuee>2304</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>yarn.app.mapreduce.am.staging-dir</key>
      <valuee>/user</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>yarn.app.mapreduce.client.job.max-retries</key>
      <valuee>60</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>yarn.application.classpath</key>
      <valuee>/etc/hadoop/conf,{{hadoop_home}}/*,{{hadoop_home}}/lib/*,/usr/hdp/current/hadoop-hdfs-client/*,/usr/hdp/current/hadoop-hdfs-client/lib/*,/usr/hdp/current/hadoop-yarn-client/*,/usr/hdp/current/hadoop-yarn-client/lib/*,/usr/hdp/current/ext/hadoop/*</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>yarn.authorization-provider</key>
      <valuee>org.apache.ranger.authorization.yarn.authorizer.RangerYarnAuthorizer</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>yarn.client.failover-proxy-provider</key>
      <valuee>org.apache.hadoop.yarn.client.RequestHedgingRMFailoverProxyProvider</valuee>
      <enabled>F</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>yarn.client.nodemanager-connect.max-wait-ms</key>
      <valuee>60000</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>yarn.client.nodemanager-connect.retry-interval-ms</key>
      <valuee>10000</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>yarn.http.policy</key>
      <valuee>HTTP_ONLY</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>yarn.log-aggregation-enable</key>
      <valuee>true</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>yarn.log-aggregation.file-controller.IndexedFormat.class</key>
      <valuee>org.apache.hadoop.yarn.logaggregation.filecontroller.ifile.LogAggregationIndexedFileController</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>yarn.log-aggregation.file-controller.TFile.class</key>
      <valuee>org.apache.hadoop.yarn.logaggregation.filecontroller.tfile.LogAggregationTFileController</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>yarn.log-aggregation.file-formats</key>
      <valuee>IndexedFormat,TFile</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>yarn.log-aggregation.retain-seconds</key>
      <valuee>2592000</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>yarn.log.server.url</key>
      <valuee>https://smvhortonworks:19890/jobhistory/logs</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>yarn.log.server.web-service.url</key>
      <valuee>http://smvhortonworks:8188/ws/v1/applicationhistory</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>yarn.node-labels.enabled</key>
      <valuee>false</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>yarn.node-labels.fs-store.retry-policy-spec</key>
      <valuee>2000, 500</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>yarn.node-labels.fs-store.root-dir</key>
      <valuee>/system/yarn/node-labels</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>yarn.nodemanager.address</key>
      <valuee>0.0.0.0:45454</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>yarn.nodemanager.admin-env</key>
      <valuee>MALLOC_ARENA_MAX=$MALLOC_ARENA_MAX</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>yarn.nodemanager.aux-services</key>
      <valuee>mapreduce_shuffle,spark_shuffle,spark2_shuffle</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>yarn.nodemanager.aux-services.mapreduce_shuffle.class</key>
      <valuee>org.apache.hadoop.mapred.ShuffleHandler</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>yarn.nodemanager.aux-services.spark2_shuffle.class</key>
      <valuee>org.apache.spark.network.yarn.YarnShuffleService</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>yarn.nodemanager.aux-services.spark2_shuffle.classpath</key>
      <valuee>{{stack_root}}/${hdp.version}/spark2/aux/*</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>yarn.nodemanager.aux-services.spark_shuffle.class</key>
      <valuee>org.apache.spark.network.yarn.YarnShuffleService</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>yarn.nodemanager.aux-services.spark_shuffle.classpath</key>
      <valuee>{{stack_root}}/${hdp.version}/spark2/aux/*</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>yarn.nodemanager.bind-host</key>
      <valuee>0.0.0.0</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>yarn.nodemanager.container-executor.class</key>
      <valuee>org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>yarn.nodemanager.container-metrics.unregister-delay-ms</key>
      <valuee>60000</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>yarn.nodemanager.container-monitor.interval-ms</key>
      <valuee>3000</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>yarn.nodemanager.delete.debug-delay-sec</key>
      <valuee>0</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>yarn.nodemanager.disk-health-checker.max-disk-utilization-per-disk-percentage</key>
      <valuee>95</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>yarn.nodemanager.disk-health-checker.min-free-space-per-disk-mb</key>
      <valuee>1000</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>yarn.nodemanager.disk-health-checker.min-healthy-disks</key>
      <valuee>0.25</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>yarn.nodemanager.health-checker.interval-ms</key>
      <valuee>135000</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>yarn.nodemanager.health-checker.script.timeout-ms</key>
      <valuee>60000</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>yarn.nodemanager.keytab</key>
      <valuee>/etc/security/keytabs/nm.service.keytab</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>yarn.nodemanager.kill-escape.launch-command-line</key>
      <valuee>slider-agent,LLAP</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>yarn.nodemanager.kill-escape.user</key>
      <valuee>hive</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>yarn.nodemanager.linux-container-executor.cgroups.strict-resource-usage</key>
      <valuee>false</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>yarn.nodemanager.linux-container-executor.group</key>
      <valuee>hadoop</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>yarn.nodemanager.local-dirs</key>
      <valuee>/hadoop/yarn/local</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>yarn.nodemanager.log-aggregation.compression-type</key>
      <valuee>gz</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>yarn.nodemanager.log-aggregation.debug-enabled</key>
      <valuee>false</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>yarn.nodemanager.log-aggregation.num-log-files-per-app</key>
      <valuee>336</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>yarn.nodemanager.log-aggregation.roll-monitoring-interval-seconds</key>
      <valuee>3600</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>yarn.nodemanager.log-dirs</key>
      <valuee>/hadoop/yarn/log</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>yarn.nodemanager.log.retain-seconds</key>
      <valuee>604800</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>yarn.nodemanager.principal</key>
      <valuee>nm/_HOST@EDAR40.EUS</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>yarn.nodemanager.recovery.dir</key>
      <valuee>{{yarn_log_dir_prefix}}/nodemanager/recovery-state</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>yarn.nodemanager.recovery.enabled</key>
      <valuee>true</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>yarn.nodemanager.remote-app-log-dir</key>
      <valuee>/app-logs</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>yarn.nodemanager.remote-app-log-dir-suffix</key>
      <valuee>logs</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>yarn.nodemanager.resource.cpu-vcores</key>
      <valuee>1</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>yarn.nodemanager.resource.memory-mb</key>
      <valuee>9216</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>yarn.nodemanager.resource.percentage-physical-cpu-limit</key>
      <valuee>80</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>yarn.nodemanager.vmem-check-enabled</key>
      <valuee>false</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>yarn.nodemanager.vmem-pmem-ratio</key>
      <valuee>2.1</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>yarn.nodemanager.webapp.spnego-keytab-file</key>
      <valuee>/etc/security/keytabs/spnego.service.keytab</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>yarn.nodemanager.webapp.spnego-principal</key>
      <valuee>HTTP/_HOST@EDAR40.EUS</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>yarn.resourcemanager.address</key>
      <valuee>smvhortonworks:8050</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>yarn.resourcemanager.admin.address</key>
      <valuee>smvhortonworks:8141</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>yarn.resourcemanager.am.max-attempts</key>
      <valuee>2</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>yarn.resourcemanager.bind-host</key>
      <valuee>0.0.0.0</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>yarn.resourcemanager.connect.max-wait.ms</key>
      <valuee>-1</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>yarn.resourcemanager.connect.retry-interval.ms</key>
      <valuee>15000</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>yarn.resourcemanager.fs.state-store.retry-policy-spec</key>
      <valuee>2000, 500</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>yarn.resourcemanager.fs.state-store.uri</key>
      <valuee> </valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>yarn.resourcemanager.ha.enabled</key>
      <valuee>false</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>yarn.resourcemanager.hostname</key>
      <valuee>smvhortonworks</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>yarn.resourcemanager.keytab</key>
      <valuee>/etc/security/keytabs/rm.service.keytab</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>yarn.resourcemanager.monitor.capacity.preemption.natural_termination_factor</key>
      <valuee>1</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>yarn.resourcemanager.monitor.capacity.preemption.total_preemption_per_round</key>
      <valuee>1</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>yarn.resourcemanager.nodes.exclude-path</key>
      <valuee>/etc/hadoop/conf/yarn.exclude</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>yarn.resourcemanager.principal</key>
      <valuee>rm/_HOST@EDAR40.EUS</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>yarn.resourcemanager.proxy-user-privileges.enabled</key>
      <valuee>true</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>yarn.resourcemanager.proxyuser.*.groups</key>
      <valuee/>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>yarn.resourcemanager.proxyuser.*.hosts</key>
      <valuee/>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>yarn.resourcemanager.proxyuser.*.users</key>
      <valuee/>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>yarn.resourcemanager.recovery.enabled</key>
      <valuee>true</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>yarn.resourcemanager.resource-tracker.address</key>
      <valuee>smvhortonworks:8025</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>yarn.resourcemanager.scheduler.address</key>
      <valuee>smvhortonworks:8030</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>yarn.resourcemanager.scheduler.class</key>
      <valuee>org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>yarn.resourcemanager.scheduler.monitor.enable</key>
      <valuee>false</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>yarn.resourcemanager.state-store.max-completed-applications</key>
      <valuee>${yarn.resourcemanager.max-completed-applications}</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>yarn.resourcemanager.store.class</key>
      <valuee>org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>yarn.resourcemanager.system-metrics-publisher.dispatcher.pool-size</key>
      <valuee>10</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>yarn.resourcemanager.system-metrics-publisher.enabled</key>
      <valuee>true</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>yarn.resourcemanager.webapp.address</key>
      <valuee>smvhortonworks:8088</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>yarn.resourcemanager.webapp.delegation-token-auth-filter.enabled</key>
      <valuee>false</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>yarn.resourcemanager.webapp.https.address</key>
      <valuee>smvhortonworks:8090</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>yarn.resourcemanager.webapp.spnego-keytab-file</key>
      <valuee>/etc/security/keytabs/spnego.service.keytab</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>yarn.resourcemanager.webapp.spnego-principal</key>
      <valuee>HTTP/_HOST@EDAR40.EUS</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>yarn.resourcemanager.work-preserving-recovery.enabled</key>
      <valuee>true</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>yarn.resourcemanager.work-preserving-recovery.scheduling-wait-ms</key>
      <valuee>10000</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>yarn.resourcemanager.zk-acl</key>
      <valuee>sasl:rm:rwcda</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>yarn.resourcemanager.zk-address</key>
      <valuee>smvhortonworks:2181</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>yarn.resourcemanager.zk-num-retries</key>
      <valuee>1000</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>yarn.resourcemanager.zk-retry-interval-ms</key>
      <valuee>1000</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>yarn.resourcemanager.zk-state-store.parent-path</key>
      <valuee>/rmstore</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>yarn.resourcemanager.zk-timeout-ms</key>
      <valuee>10000</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>yarn.scheduler.capacity.ordering-policy.priority-utilization.underutilized-preemption.enabled</key>
      <valuee>false</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>yarn.scheduler.maximum-allocation-mb</key>
      <valuee>9216</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>yarn.scheduler.maximum-allocation-vcores</key>
      <valuee>1</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>yarn.scheduler.minimum-allocation-mb</key>
      <valuee>2304</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>yarn.scheduler.minimum-allocation-vcores</key>
      <valuee>1</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>yarn.timeline-service.address</key>
      <valuee>smvhortonworks:10200</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>yarn.timeline-service.bind-host</key>
      <valuee>0.0.0.0</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>yarn.timeline-service.client.fd-flush-interval-secs</key>
      <valuee>5</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>yarn.timeline-service.client.max-retries</key>
      <valuee>30</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>yarn.timeline-service.client.retry-interval-ms</key>
      <valuee>1000</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>yarn.timeline-service.enabled</key>
      <valuee>true</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>yarn.timeline-service.entity-group-fs-store.active-dir</key>
      <valuee>/ats/active/</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>yarn.timeline-service.entity-group-fs-store.app-cache-size</key>
      <valuee>10</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>yarn.timeline-service.entity-group-fs-store.cleaner-interval-seconds</key>
      <valuee>3600</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>yarn.timeline-service.entity-group-fs-store.done-dir</key>
      <valuee>/ats/done/</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>yarn.timeline-service.entity-group-fs-store.group-id-plugin-classes</key>
      <valuee>org.apache.tez.dag.history.logging.ats.TimelineCachePluginImpl</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>yarn.timeline-service.entity-group-fs-store.group-id-plugin-classpath</key>
      <valuee>/usr/hdp/${hdp.version}/spark2/jars/*</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>yarn.timeline-service.entity-group-fs-store.retain-seconds</key>
      <valuee>604800</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>yarn.timeline-service.entity-group-fs-store.scan-interval-seconds</key>
      <valuee>15</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>yarn.timeline-service.entity-group-fs-store.summary-store</key>
      <valuee>org.apache.hadoop.yarn.server.timeline.RollingLevelDBTimelineStore</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>yarn.timeline-service.generic-application-history.store-class</key>
      <valuee>org.apache.hadoop.yarn.server.applicationhistoryservice.NullApplicationHistoryStore</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>yarn.timeline-service.http-authentication.cookie.domain</key>
      <valuee/>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>yarn.timeline-service.http-authentication.cookie.path</key>
      <valuee/>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>yarn.timeline-service.http-authentication.kerberos.keytab</key>
      <valuee>/etc/security/keytabs/spnego.service.keytab</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>yarn.timeline-service.http-authentication.kerberos.name.rules</key>
      <valuee/>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>yarn.timeline-service.http-authentication.kerberos.principal</key>
      <valuee>HTTP/_HOST@EDAR40.EUS</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>yarn.timeline-service.http-authentication.proxyuser.*.groups</key>
      <valuee/>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>yarn.timeline-service.http-authentication.proxyuser.*.hosts</key>
      <valuee/>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>yarn.timeline-service.http-authentication.proxyuser.*.users</key>
      <valuee/>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>yarn.timeline-service.http-authentication.proxyuser.ambari-server-edar.groups</key>
      <valuee>*</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>yarn.timeline-service.http-authentication.proxyuser.ambari-server-edar.hosts</key>
      <valuee>smvhortonworks</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>yarn.timeline-service.http-authentication.signature.secret</key>
      <valuee/>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>yarn.timeline-service.http-authentication.signature.secret.file</key>
      <valuee/>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>yarn.timeline-service.http-authentication.signer.secret.provider</key>
      <valuee/>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>yarn.timeline-service.http-authentication.signer.secret.provider.object</key>
      <valuee/>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>yarn.timeline-service.http-authentication.simple.anonymous.allowed</key>
      <valuee>true</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>yarn.timeline-service.http-authentication.token.validity</key>
      <valuee/>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>yarn.timeline-service.http-authentication.type</key>
      <valuee>kerberos</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>yarn.timeline-service.keytab</key>
      <valuee>/etc/security/keytabs/yarn.service.keytab</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>yarn.timeline-service.leveldb-state-store.path</key>
      <valuee>/hadoop/yarn/timeline</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>yarn.timeline-service.leveldb-timeline-store.path</key>
      <valuee>/hadoop/yarn/timeline</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>yarn.timeline-service.leveldb-timeline-store.read-cache-size</key>
      <valuee>104857600</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>yarn.timeline-service.leveldb-timeline-store.start-time-read-cache-size</key>
      <valuee>10000</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>yarn.timeline-service.leveldb-timeline-store.start-time-write-cache-size</key>
      <valuee>10000</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>yarn.timeline-service.leveldb-timeline-store.ttl-interval-ms</key>
      <valuee>300000</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>yarn.timeline-service.principal</key>
      <valuee>yarn/_HOST@EDAR40.EUS</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>yarn.timeline-service.recovery.enabled</key>
      <valuee>true</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>yarn.timeline-service.state-store-class</key>
      <valuee>org.apache.hadoop.yarn.server.timeline.recovery.LeveldbTimelineStateStore</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>yarn.timeline-service.store-class</key>
      <valuee>org.apache.hadoop.yarn.server.timeline.EntityGroupFSTimelineStore</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>yarn.timeline-service.ttl-enable</key>
      <valuee>true</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>yarn.timeline-service.ttl-ms</key>
      <valuee>2678400000</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>yarn.timeline-service.version</key>
      <valuee>1.5</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>yarn.timeline-service.webapp.address</key>
      <valuee>smvhortonworks:8188</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>yarn.timeline-service.webapp.https.address</key>
      <valuee>smvhortonworks:8190</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
  </advancedHadoopSettings>
  <advancedHiveSettings>
    <keyvalueenabledelement>
      <key>ambari.hive.db.schema.name</key>
      <valuee>hive</valuee>
      <enabled>F</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>atlas.hook.hive.maxThreads</key>
      <valuee>1</valuee>
      <enabled>F</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>atlas.hook.hive.minThreads</key>
      <valuee>1</valuee>
      <enabled>F</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>datanucleus.autoCreateSchema</key>
      <valuee>false</valuee>
      <enabled>F</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>datanucleus.cache.level2.type</key>
      <valuee>none</valuee>
      <enabled>F</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>datanucleus.fixedDatastore</key>
      <valuee>true</valuee>
      <enabled>F</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>hive.auto.convert.join</key>
      <valuee>true</valuee>
      <enabled>F</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>hive.auto.convert.join.noconditionaltask</key>
      <valuee>true</valuee>
      <enabled>F</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>hive.auto.convert.join.noconditionaltask.size</key>
      <valuee>644245094</valuee>
      <enabled>F</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>hive.auto.convert.sortmerge.join</key>
      <valuee>true</valuee>
      <enabled>F</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>hive.auto.convert.sortmerge.join.to.mapjoin</key>
      <valuee>false</valuee>
      <enabled>F</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>hive.cbo.enable</key>
      <valuee>true</valuee>
      <enabled>F</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>hive.cli.print.header</key>
      <valuee>false</valuee>
      <enabled>F</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>hive.cluster.delegation.token.store.class</key>
      <valuee>org.apache.hadoop.hive.thrift.ZooKeeperTokenStore</valuee>
      <enabled>F</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>hive.cluster.delegation.token.store.zookeeper.connectString</key>
      <valuee>smvhortonworks:2181</valuee>
      <enabled>F</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>hive.cluster.delegation.token.store.zookeeper.znode</key>
      <valuee>/hive/cluster/delegation</valuee>
      <enabled>F</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>hive.compactor.abortedtxn.threshold</key>
      <valuee>1000</valuee>
      <enabled>F</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>hive.compactor.check.interval</key>
      <valuee>300L</valuee>
      <enabled>F</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>hive.compactor.delta.num.threshold</key>
      <valuee>10</valuee>
      <enabled>F</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>hive.compactor.delta.pct.threshold</key>
      <valuee>0.1f</valuee>
      <enabled>F</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>hive.compactor.initiator.on</key>
      <valuee>false</valuee>
      <enabled>F</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>hive.compactor.worker.threads</key>
      <valuee>0</valuee>
      <enabled>F</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>hive.compactor.worker.timeout</key>
      <valuee>86400L</valuee>
      <enabled>F</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>hive.compute.query.using.stats</key>
      <valuee>true</valuee>
      <enabled>F</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>hive.conf.restricted.list</key>
      <valuee>hive.security.authenticator.manager,hive.security.authorization.manager,hive.users.in.admin.role</valuee>
      <enabled>F</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>hive.convert.join.bucket.mapjoin.tez</key>
      <valuee>false</valuee>
      <enabled>F</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>hive.custom-extensions.root</key>
      <valuee>/hdp/ext/{{major_stack_version}}/hive</valuee>
      <enabled>F</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>hive.default.fileformat</key>
      <valuee>TextFile</valuee>
      <enabled>F</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>hive.default.fileformat.managed</key>
      <valuee>TextFile</valuee>
      <enabled>F</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>hive.enforce.bucketing</key>
      <valuee>true</valuee>
      <enabled>F</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>hive.enforce.sorting</key>
      <valuee>true</valuee>
      <enabled>F</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>hive.enforce.sortmergebucketmapjoin</key>
      <valuee>true</valuee>
      <enabled>F</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>hive.exec.compress.intermediate</key>
      <valuee>false</valuee>
      <enabled>F</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>hive.exec.compress.output</key>
      <valuee>false</valuee>
      <enabled>F</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>hive.exec.dynamic.partition</key>
      <valuee>true</valuee>
      <enabled>F</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>hive.exec.dynamic.partition.mode</key>
      <valuee>strict</valuee>
      <enabled>F</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>hive.exec.failure.hooks</key>
      <valuee>org.apache.hadoop.hive.ql.hooks.ATSHook</valuee>
      <enabled>F</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>hive.exec.max.created.files</key>
      <valuee>100000</valuee>
      <enabled>F</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>hive.exec.max.dynamic.partitions</key>
      <valuee>5000</valuee>
      <enabled>F</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>hive.exec.max.dynamic.partitions.pernode</key>
      <valuee>2000</valuee>
      <enabled>F</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>hive.exec.orc.compression.strategy</key>
      <valuee>SPEED</valuee>
      <enabled>F</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>hive.exec.orc.default.compress</key>
      <valuee>ZLIB</valuee>
      <enabled>F</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>hive.exec.orc.default.stripe.size</key>
      <valuee>67108864</valuee>
      <enabled>F</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>hive.exec.orc.encoding.strategy</key>
      <valuee>SPEED</valuee>
      <enabled>F</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>hive.exec.parallel</key>
      <valuee>false</valuee>
      <enabled>F</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>hive.exec.parallel.thread.number</key>
      <valuee>8</valuee>
      <enabled>F</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>hive.exec.post.hooks</key>
      <valuee>org.apache.hadoop.hive.ql.hooks.ATSHook</valuee>
      <enabled>F</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>hive.exec.pre.hooks</key>
      <valuee>org.apache.hadoop.hive.ql.hooks.ATSHook</valuee>
      <enabled>F</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>hive.exec.reducers.bytes.per.reducer</key>
      <valuee>67108864</valuee>
      <enabled>F</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>hive.exec.reducers.max</key>
      <valuee>1009</valuee>
      <enabled>F</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>hive.exec.scratchdir</key>
      <valuee>/tmp/hive</valuee>
      <enabled>F</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>hive.exec.submit.local.task.via.child</key>
      <valuee>true</valuee>
      <enabled>F</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>hive.exec.submitviachild</key>
      <valuee>false</valuee>
      <enabled>F</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>hive.execution.engine</key>
      <valuee>tez</valuee>
      <enabled>F</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>hive.fetch.task.aggr</key>
      <valuee>false</valuee>
      <enabled>F</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>hive.fetch.task.conversion</key>
      <valuee>more</valuee>
      <enabled>F</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>hive.fetch.task.conversion.threshold</key>
      <valuee>1073741824</valuee>
      <enabled>F</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>hive.limit.optimize.enable</key>
      <valuee>true</valuee>
      <enabled>F</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>hive.limit.pushdown.memory.usage</key>
      <valuee>0.04</valuee>
      <enabled>F</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>hive.map.aggr</key>
      <valuee>true</valuee>
      <enabled>F</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>hive.map.aggr.hash.force.flush.memory.threshold</key>
      <valuee>0.9</valuee>
      <enabled>F</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>hive.map.aggr.hash.min.reduction</key>
      <valuee>0.5</valuee>
      <enabled>F</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>hive.map.aggr.hash.percentmemory</key>
      <valuee>0.5</valuee>
      <enabled>F</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>hive.mapjoin.bucket.cache.size</key>
      <valuee>10000</valuee>
      <enabled>F</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>hive.mapjoin.optimized.hashtable</key>
      <valuee>true</valuee>
      <enabled>F</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>hive.mapred.reduce.tasks.speculative.execution</key>
      <valuee>false</valuee>
      <enabled>F</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>hive.merge.mapfiles</key>
      <valuee>true</valuee>
      <enabled>F</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>hive.merge.mapredfiles</key>
      <valuee>false</valuee>
      <enabled>F</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>hive.merge.orcfile.stripe.level</key>
      <valuee>true</valuee>
      <enabled>F</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>hive.merge.rcfile.block.level</key>
      <valuee>true</valuee>
      <enabled>F</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>hive.merge.size.per.task</key>
      <valuee>256000000</valuee>
      <enabled>F</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>hive.merge.smallfiles.avgsize</key>
      <valuee>16000000</valuee>
      <enabled>F</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>hive.merge.tezfiles</key>
      <valuee>true</valuee>
      <enabled>F</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>hive.metastore.authorization.storage.checks</key>
      <valuee>false</valuee>
      <enabled>F</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>hive.metastore.cache.pinobjtypes</key>
      <valuee>Table,Database,Type,FieldSchema,Order</valuee>
      <enabled>F</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>hive.metastore.client.connect.retry.delay</key>
      <valuee>5s</valuee>
      <enabled>F</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>hive.metastore.client.socket.timeout</key>
      <valuee>1800s</valuee>
      <enabled>F</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>hive.metastore.connect.retries</key>
      <valuee>24</valuee>
      <enabled>F</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>hive.metastore.execute.setugi</key>
      <valuee>true</valuee>
      <enabled>F</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>hive.metastore.failure.retries</key>
      <valuee>24</valuee>
      <enabled>F</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>hive.metastore.kerberos.keytab.file</key>
      <valuee>/etc/security/keytabs/hive.service.keytab</valuee>
      <enabled>F</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>hive.metastore.kerberos.principal</key>
      <valuee>hive/_HOST@EDAR40.EUS</valuee>
      <enabled>F</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>hive.metastore.pre.event.listeners</key>
      <valuee>org.apache.hadoop.hive.ql.security.authorization.AuthorizationPreEventListener</valuee>
      <enabled>F</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>hive.metastore.sasl.enabled</key>
      <valuee>true</valuee>
      <enabled>F</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>hive.metastore.server.max.threads</key>
      <valuee>100000</valuee>
      <enabled>F</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>hive.metastore.uris</key>
      <valuee>thrift://smvhortonworks:9083</valuee>
      <enabled>F</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>hive.metastore.warehouse.dir</key>
      <valuee>/apps/hive/warehouse</valuee>
      <enabled>F</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>hive.optimize.bucketmapjoin</key>
      <valuee>true</valuee>
      <enabled>F</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>hive.optimize.bucketmapjoin.sortedmerge</key>
      <valuee>false</valuee>
      <enabled>F</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>hive.optimize.constant.propagation</key>
      <valuee>true</valuee>
      <enabled>F</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>hive.optimize.index.filter</key>
      <valuee>true</valuee>
      <enabled>F</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>hive.optimize.metadataonly</key>
      <valuee>true</valuee>
      <enabled>F</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>hive.optimize.null.scan</key>
      <valuee>true</valuee>
      <enabled>F</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>hive.optimize.reducededuplication</key>
      <valuee>true</valuee>
      <enabled>F</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>hive.optimize.reducededuplication.min.reducer</key>
      <valuee>4</valuee>
      <enabled>F</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>hive.optimize.sort.dynamic.partition</key>
      <valuee>false</valuee>
      <enabled>F</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>hive.orc.compute.splits.num.threads</key>
      <valuee>10</valuee>
      <enabled>F</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>hive.orc.splits.include.file.footer</key>
      <valuee>false</valuee>
      <enabled>F</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>hive.prewarm.enabled</key>
      <valuee>false</valuee>
      <enabled>F</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>hive.prewarm.numcontainers</key>
      <valuee>3</valuee>
      <enabled>F</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>hive.security.authenticator.manager</key>
      <valuee>org.apache.hadoop.hive.ql.security.ProxyUserAuthenticator</valuee>
      <enabled>F</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>hive.security.authorization.enabled</key>
      <valuee>true</valuee>
      <enabled>F</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>hive.security.authorization.manager</key>
      <valuee>org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdConfOnlyAuthorizerFactory</valuee>
      <enabled>F</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>hive.security.metastore.authenticator.manager</key>
      <valuee>org.apache.hadoop.hive.ql.security.HadoopDefaultMetastoreAuthenticator</valuee>
      <enabled>F</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>hive.security.metastore.authorization.auth.reads</key>
      <valuee>true</valuee>
      <enabled>F</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>hive.security.metastore.authorization.manager</key>
      <valuee>org.apache.hadoop.hive.ql.security.authorization.StorageBasedAuthorizationProvider</valuee>
      <enabled>F</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>hive.server2.allow.user.substitution</key>
      <valuee>true</valuee>
      <enabled>F</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>hive.server2.authentication</key>
      <valuee>KERBEROS</valuee>
      <enabled>F</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>hive.server2.authentication.kerberos.keytab</key>
      <valuee>/etc/security/keytabs/hive.service.keytab</valuee>
      <enabled>F</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>hive.server2.authentication.kerberos.principal</key>
      <valuee>hive/_HOST@EDAR40.EUS</valuee>
      <enabled>F</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>hive.server2.authentication.spnego.keytab</key>
      <valuee>/etc/security/keytabs/spnego.service.keytab</valuee>
      <enabled>F</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>hive.server2.authentication.spnego.principal</key>
      <valuee>HTTP/_HOST@EDAR40.EUS</valuee>
      <enabled>F</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>hive.server2.enable.doAs</key>
      <valuee>true</valuee>
      <enabled>F</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>hive.server2.logging.operation.enabled</key>
      <valuee>true</valuee>
      <enabled>F</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>hive.server2.logging.operation.log.location</key>
      <valuee>/tmp/hive/operation_logs</valuee>
      <enabled>F</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>hive.server2.max.start.attempts</key>
      <valuee>5</valuee>
      <enabled>F</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>hive.server2.support.dynamic.service.discovery</key>
      <valuee>true</valuee>
      <enabled>F</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>hive.server2.table.type.mapping</key>
      <valuee>CLASSIC</valuee>
      <enabled>F</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>hive.server2.tez.default.queues</key>
      <valuee>default</valuee>
      <enabled>F</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>hive.server2.tez.initialize.default.sessions</key>
      <valuee>false</valuee>
      <enabled>F</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>hive.server2.tez.sessions.per.default.queue</key>
      <valuee>1</valuee>
      <enabled>F</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>hive.server2.thrift.http.path</key>
      <valuee>cliservice</valuee>
      <enabled>F</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>hive.server2.thrift.http.port</key>
      <valuee>10001</valuee>
      <enabled>F</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>hive.server2.thrift.max.worker.threads</key>
      <valuee>500</valuee>
      <enabled>F</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>hive.server2.thrift.port</key>
      <valuee>10000</valuee>
      <enabled>F</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>hive.server2.thrift.sasl.qop</key>
      <valuee>auth</valuee>
      <enabled>F</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>hive.server2.transport.mode</key>
      <valuee>binary</valuee>
      <enabled>F</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>hive.server2.use.SSL</key>
      <valuee>false</valuee>
      <enabled>F</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>hive.server2.zookeeper.namespace</key>
      <valuee>hiveserver2</valuee>
      <enabled>F</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>hive.smbjoin.cache.rows</key>
      <valuee>10000</valuee>
      <enabled>F</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>hive.start.cleanup.scratchdir</key>
      <valuee>false</valuee>
      <enabled>F</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>hive.stats.autogather</key>
      <valuee>true</valuee>
      <enabled>F</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>hive.stats.dbclass</key>
      <valuee>fs</valuee>
      <enabled>F</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>hive.stats.fetch.column.stats</key>
      <valuee>true</valuee>
      <enabled>F</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>hive.stats.fetch.partition.stats</key>
      <valuee>true</valuee>
      <enabled>F</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>hive.support.concurrency</key>
      <valuee>false</valuee>
      <enabled>F</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>hive.tez.auto.reducer.parallelism</key>
      <valuee>true</valuee>
      <enabled>F</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>hive.tez.container.size</key>
      <valuee>2304</valuee>
      <enabled>F</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>hive.tez.cpu.vcores</key>
      <valuee>-1</valuee>
      <enabled>F</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>hive.tez.dynamic.partition.pruning</key>
      <valuee>true</valuee>
      <enabled>F</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>hive.tez.dynamic.partition.pruning.max.data.size</key>
      <valuee>104857600</valuee>
      <enabled>F</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>hive.tez.dynamic.partition.pruning.max.event.size</key>
      <valuee>1048576</valuee>
      <enabled>F</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>hive.tez.input.format</key>
      <valuee>org.apache.hadoop.hive.ql.io.HiveInputFormat</valuee>
      <enabled>F</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>hive.tez.java.opts</key>
      <valuee>-server -Djava.net.preferIPv4Stack=true -XX:NewRatio=8 -XX:+UseNUMA -XX:+UseG1GC -XX:+ResizeTLAB -XX:+PrintGCDetails -verbose:gc -XX:+PrintGCTimeStamps</valuee>
      <enabled>F</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>hive.tez.log.level</key>
      <valuee>INFO</valuee>
      <enabled>F</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>hive.tez.max.partition.factor</key>
      <valuee>2.0</valuee>
      <enabled>F</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>hive.tez.min.partition.factor</key>
      <valuee>0.25</valuee>
      <enabled>F</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>hive.tez.smb.number.waves</key>
      <valuee>0.5</valuee>
      <enabled>F</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>hive.txn.manager</key>
      <valuee>org.apache.hadoop.hive.ql.lockmgr.DummyTxnManager</valuee>
      <enabled>F</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>hive.txn.max.open.batch</key>
      <valuee>1000</valuee>
      <enabled>F</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>hive.txn.timeout</key>
      <valuee>300</valuee>
      <enabled>F</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>hive.user.install.directory</key>
      <valuee>/user/</valuee>
      <enabled>F</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>hive.vectorized.execution.enabled</key>
      <valuee>true</valuee>
      <enabled>F</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>hive.vectorized.execution.reduce.enabled</key>
      <valuee>false</valuee>
      <enabled>F</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>hive.vectorized.groupby.checkinterval</key>
      <valuee>4096</valuee>
      <enabled>F</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>hive.vectorized.groupby.flush.percent</key>
      <valuee>0.1</valuee>
      <enabled>F</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>hive.vectorized.groupby.maxentries</key>
      <valuee>100000</valuee>
      <enabled>F</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>hive.warehouse.subdir.inherit.perms</key>
      <valuee>true</valuee>
      <enabled>F</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>hive.zookeeper.client.port</key>
      <valuee>2181</valuee>
      <enabled>F</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>hive.zookeeper.namespace</key>
      <valuee>hive_zookeeper_namespace</valuee>
      <enabled>F</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>hive.zookeeper.quorum</key>
      <valuee>smvhortonworks:2181</valuee>
      <enabled>F</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>javax.jdo.option.ConnectionDriverName</key>
      <valuee>com.mysql.jdbc.Driver</valuee>
      <enabled>F</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>javax.jdo.option.ConnectionPassword</key>
      <valuee>SECRET:hive-site:57:javax.jdo.option.ConnectionPassword</valuee>
      <enabled>F</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>javax.jdo.option.ConnectionURL</key>
      <valuee>jdbc:mysql://localhost/hive</valuee>
      <enabled>F</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>javax.jdo.option.ConnectionUserName</key>
      <valuee>hive</valuee>
      <enabled>F</enabled>
    </keyvalueenabledelement>
  </advancedHiveSettings>
  <advancedSparkSettings>
    <keyvalueenabledelement>
      <key>spark.authenticate</key>
      <valuee>true</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>spark.authenticate.enableSaslEncryption</key>
      <valuee>true</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>spark.driver.extraLibraryPath</key>
      <valuee>/usr/hdp/current/hadoop-client/lib/native:/usr/hdp/current/hadoop-client/lib/native/Linux-amd64-64</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>spark.eventLog.dir</key>
      <valuee>hdfs:///spark2-history/</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>spark.eventLog.enabled</key>
      <valuee>true</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>spark.executor.extraLibraryPath</key>
      <valuee>/usr/hdp/current/hadoop-client/lib/native:/usr/hdp/current/hadoop-client/lib/native/Linux-amd64-64</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>spark.history.fs.logDirectory</key>
      <valuee>hdfs://spark2-history/</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>spark.history.kerberos.enabled</key>
      <valuee>true</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>spark.history.kerberos.keytab</key>
      <valuee>/etc/security/keytabs/spark.headless.keytab</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>spark.history.kerberos.principal</key>
      <valuee>spark-edar@EDAR40.EUS</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>spark.history.provider</key>
      <valuee>org.apache.spark.deploy.history.FsHistoryProvider</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>spark.history.ui.port</key>
      <valuee>18081</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>spark.yarn.historyServer.address</key>
      <valuee>smvhortonworks:18081</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
    <keyvalueenabledelement>
      <key>spark.yarn.queue</key>
      <valuee>default</valuee>
      <enabled>T</enabled>
    </keyvalueenabledelement>
  </advancedSparkSettings>
  <libdir/>
  <hiveusername>hive</hiveusername>
  <hiveversion>hive2</hiveversion>
  <hivepassword>zz8MX9PUxAI=_</hivepassword>
  <manuallyinstalledudfs>F</manuallyinstalledudfs>
  <usecustomudfdatabase>F</usecustomudfdatabase>
  <customudfdatabase/>
  <hiveurlpostfix/>
  <hivejdbc>hive_0.13.0</hivejdbc>
  <execframework>yarn</execframework>
  <accesswhitelist>*</accesswhitelist>
  <forceproxyonserver>F</forceproxyonserver>
  <useContainerPool>T</useContainerPool>
  <maprSecurityEnabled>F</maprSecurityEnabled>
  <maprClusterId/>
</radoop-connection-entry>
